{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T20:33:45.778526Z",
     "start_time": "2020-12-08T20:33:45.106547Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T22:16:43.710204Z",
     "start_time": "2020-12-08T22:16:43.690177Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Getting the stemmer and stopwords to be removed\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop = stopwords.words('english')\n",
    "stop.append('@')\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "\n",
    "#Some Preprocessing Functions\n",
    "def remove_digits(string):\n",
    "    return re.sub('[^a-zA-Z]', ' ', string )\n",
    "\n",
    "def remove_special_chars(string):\n",
    "    return re.sub(r'\\s+', ' ', string)\n",
    "\n",
    "#https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python\n",
    "def prepare_corpus(doc_clean):\n",
    "    \"\"\"\n",
    "    Input  : clean document\n",
    "    Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\n",
    "    Output : term dictionary and Document Term Matrix\n",
    "    \"\"\"\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    # generate LDA model\n",
    "    return dictionary,doc_term_matrix\n",
    "\n",
    "# A function to get the Latent Semantic Analysis of the words given to it, and it makes them into 3 topics\n",
    "def train_and_print_topics(comment_clean):\n",
    "    dictionary,doc_term_matrix=prepare_corpus(comment_clean)\n",
    "    lsamodel = LsiModel(doc_term_matrix, num_topics=3, id2word = dictionary) \n",
    "    topics = lsamodel.print_topics(num_topics=3, num_words=50)\n",
    "    for i in range(len(topics)):\n",
    "        print(\"Topic\",i+1,\":\",topics[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T21:09:35.042693Z",
     "start_time": "2020-12-08T21:09:04.865380Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reading the dataset file, then Preprocessing the comments column \n",
    "\n",
    "df= pd.read_excel('AllchemtrailsComments.xlsx').dropna()\n",
    "\n",
    "\n",
    "df['comment'] = df['comment'].apply(remove_digits)\n",
    "df['comment'] = df['comment'].apply(remove_special_chars)\n",
    "\n",
    "\n",
    "df['comment'] = df['comment'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "df['comment'] = df['comment'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "df['comment'] = df['comment'].apply(lambda x: ' '.join([word.lower() for word in x.split() ]))\n",
    "\n",
    "comment_clean = df['comment']\n",
    "\n",
    "comment_clean = [string.split() for string in comment_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T22:19:35.536830Z",
     "start_time": "2020-12-08T22:16:53.721403Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 : 0.956*\"u\" + 0.281*\"f\" + 0.054*\"e\" + 0.038*\"b\" + 0.034*\"c\" + 0.021*\"n\" + 0.014*\"aa\" + 0.007*\"ufe\" + 0.003*\"i\" + 0.002*\"fd\" + 0.002*\"uff\" + 0.002*\"fa\" + 0.002*\"fc\" + 0.002*\"fb\" + 0.002*\"ni\" + 0.002*\"fe\" + 0.001*\"ff\" + 0.001*\"ea\" + 0.001*\"ec\" + 0.001*\"ed\" + 0.001*\"xa\" + 0.001*\"eba\" + 0.001*\"cc\" + 0.001*\"ee\" + 0.001*\"d\" + 0.001*\"bb\" + 0.001*\"xbb\" + 0.001*\"xab\" + 0.001*\"ad\" + 0.001*\"bd\" + 0.001*\"go\" + 0.001*\"cf\" + 0.001*\"af\" + 0.001*\"s\" + 0.001*\"ab\" + 0.001*\"sleep\" + 0.000*\"tonight\" + 0.000*\"bore\" + 0.000*\"eb\" + 0.000*\"cd\" + 0.000*\"ef\" + 0.000*\"ba\" + 0.000*\"bomber\" + 0.000*\"scare\" + 0.000*\"nightmar\" + 0.000*\"cb\" + 0.000*\"de\" + 0.000*\"peopl\" + 0.000*\"dd\" + 0.000*\"the\"\n",
      "Topic 2 : -0.957*\"f\" + 0.280*\"u\" + -0.055*\"aa\" + 0.035*\"e\" + -0.026*\"ufe\" + 0.013*\"c\" + 0.011*\"n\" + -0.007*\"fc\" + -0.007*\"fb\" + -0.006*\"fe\" + -0.006*\"b\" + -0.006*\"fa\" + -0.006*\"ff\" + -0.006*\"fd\" + -0.005*\"die\" + -0.004*\"go\" + -0.004*\"nim\" + -0.004*\"cking\" + -0.004*\"ec\" + 0.004*\"ni\" + 0.003*\"xa\" + 0.002*\"uff\" + -0.002*\"ee\" + -0.002*\"ea\" + 0.002*\"scare\" + -0.002*\"cf\" + -0.002*\"wiggl\" + 0.002*\"d\" + 0.001*\"xab\" + 0.001*\"xbb\" + -0.001*\"eb\" + 0.001*\"r\" + 0.001*\"eba\" + 0.001*\"s\" + -0.001*\"ef\" + 0.001*\"bomber\" + -0.001*\"dd\" + -0.001*\"ab\" + -0.001*\"de\" + -0.001*\"ce\" + -0.001*\"ed\" + -0.001*\"bf\" + -0.000*\"bc\" + -0.000*\"like\" + -0.000*\"cb\" + -0.000*\"xe\" + -0.000*\"ae\" + -0.000*\"da\" + 0.000*\"muslim\" + -0.000*\"nwe\"\n",
      "Topic 3 : 0.764*\"xa\" + 0.495*\"n\" + 0.338*\"wiggl\" + 0.186*\"wrong\" + 0.093*\"i\" + 0.056*\"scare\" + 0.053*\"d\" + 0.041*\"ni\" + 0.029*\"s\" + 0.027*\"bomber\" + 0.026*\"nwiggl\" + 0.025*\"im\" + 0.023*\"c\" + 0.017*\"r\" + -0.015*\"u\" + 0.014*\"e\" + -0.013*\"aa\" + 0.008*\"b\" + 0.007*\"bore\" + 0.006*\"wigl\" + 0.006*\"comment\" + 0.006*\"the\" + 0.006*\"safe\" + 0.005*\"wtc\" + 0.005*\"build\" + 0.005*\"plane\" + 0.005*\"would\" + 0.005*\"f\" + 0.004*\"peopl\" + 0.004*\"nim\" + 0.004*\"one\" + 0.003*\"go\" + 0.003*\"like\" + 0.003*\"tower\" + 0.003*\"ufe\" + 0.003*\"collaps\" + 0.003*\"nthe\" + 0.003*\"flight\" + 0.002*\"time\" + 0.002*\"say\" + 0.002*\"steel\" + 0.002*\"video\" + 0.002*\"even\" + 0.002*\"fire\" + 0.002*\"day\" + 0.002*\"know\" + 0.002*\"attack\" + 0.002*\"a\" + 0.002*\"it\" + 0.002*\"said\"\n"
     ]
    }
   ],
   "source": [
    "train_and_print_topics(comment_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T22:43:39.115453Z",
     "start_time": "2020-12-08T22:39:40.015363Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reading the dataset file, then Preprocessing the comments column \n",
    "\n",
    "df= pd.read_excel('All911Comments.xlsx').dropna()\n",
    "\n",
    "df['comment'] = df['comment'].apply(remove_digits)\n",
    "# df['comment'] = df['comment'].apply(remove_special_chars)\n",
    "\n",
    "df['comment'] = df['comment'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "df['comment'] = df['comment'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "df['comment'] = df['comment'].apply(lambda x: ' '.join([word.lower() for word in x.split() ]))\n",
    "\n",
    "comment_clean = df['comment']\n",
    "\n",
    "comment_clean = [string.split() for string in comment_clean]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T22:45:56.410854Z",
     "start_time": "2020-12-08T22:43:45.722987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 : 0.956*\"u\" + 0.281*\"f\" + 0.054*\"e\" + 0.038*\"b\" + 0.034*\"c\" + 0.021*\"n\" + 0.014*\"aa\" + 0.007*\"ufe\" + 0.003*\"i\" + 0.002*\"fd\" + 0.002*\"uff\" + 0.002*\"fa\" + 0.002*\"fc\" + 0.002*\"fb\" + 0.002*\"ni\" + 0.002*\"fe\" + 0.001*\"ff\" + 0.001*\"ea\" + 0.001*\"ec\" + 0.001*\"ed\" + 0.001*\"xa\" + 0.001*\"eba\" + 0.001*\"cc\" + 0.001*\"ee\" + 0.001*\"d\" + 0.001*\"bb\" + 0.001*\"xbb\" + 0.001*\"xab\" + 0.001*\"ad\" + 0.001*\"bd\" + 0.001*\"go\" + 0.001*\"cf\" + 0.001*\"af\" + 0.001*\"s\" + 0.001*\"ab\" + 0.001*\"sleep\" + 0.000*\"tonight\" + 0.000*\"bore\" + 0.000*\"eb\" + 0.000*\"cd\" + 0.000*\"ef\" + 0.000*\"ba\" + 0.000*\"bomber\" + 0.000*\"scare\" + 0.000*\"nightmar\" + 0.000*\"cb\" + 0.000*\"de\" + 0.000*\"peopl\" + 0.000*\"dd\" + 0.000*\"the\"\n",
      "Topic 2 : -0.957*\"f\" + 0.280*\"u\" + -0.055*\"aa\" + 0.035*\"e\" + -0.026*\"ufe\" + 0.013*\"c\" + 0.011*\"n\" + -0.007*\"fc\" + -0.007*\"fb\" + -0.006*\"fe\" + -0.006*\"b\" + -0.006*\"fa\" + -0.006*\"ff\" + -0.006*\"fd\" + -0.005*\"die\" + -0.004*\"go\" + -0.004*\"nim\" + -0.004*\"cking\" + -0.004*\"ec\" + 0.004*\"ni\" + 0.003*\"xa\" + 0.002*\"uff\" + -0.002*\"ee\" + -0.002*\"ea\" + 0.002*\"scare\" + -0.002*\"cf\" + -0.002*\"wiggl\" + 0.002*\"d\" + 0.001*\"xab\" + 0.001*\"xbb\" + -0.001*\"eb\" + 0.001*\"r\" + 0.001*\"eba\" + 0.001*\"s\" + -0.001*\"ef\" + 0.001*\"bomber\" + -0.001*\"dd\" + -0.001*\"ab\" + -0.001*\"de\" + -0.001*\"ce\" + -0.001*\"ed\" + -0.001*\"bf\" + -0.000*\"bc\" + -0.000*\"like\" + -0.000*\"cb\" + -0.000*\"xe\" + -0.000*\"ae\" + -0.000*\"da\" + 0.000*\"muslim\" + -0.000*\"nwe\"\n",
      "Topic 3 : 0.764*\"xa\" + 0.495*\"n\" + 0.338*\"wiggl\" + 0.186*\"wrong\" + 0.093*\"i\" + 0.056*\"scare\" + 0.053*\"d\" + 0.041*\"ni\" + 0.029*\"s\" + 0.027*\"bomber\" + 0.026*\"nwiggl\" + 0.025*\"im\" + 0.023*\"c\" + 0.017*\"r\" + -0.015*\"u\" + 0.014*\"e\" + -0.013*\"aa\" + 0.008*\"b\" + 0.007*\"bore\" + 0.006*\"wigl\" + 0.006*\"comment\" + 0.006*\"the\" + 0.006*\"safe\" + 0.005*\"wtc\" + 0.005*\"build\" + 0.005*\"plane\" + 0.005*\"would\" + 0.005*\"f\" + 0.004*\"peopl\" + 0.004*\"nim\" + 0.004*\"one\" + 0.003*\"go\" + 0.003*\"like\" + 0.003*\"tower\" + 0.003*\"ufe\" + 0.003*\"collaps\" + 0.003*\"nthe\" + 0.003*\"flight\" + 0.002*\"time\" + 0.002*\"say\" + 0.002*\"steel\" + 0.002*\"video\" + 0.002*\"even\" + 0.002*\"fire\" + 0.002*\"day\" + 0.002*\"know\" + 0.002*\"attack\" + 0.002*\"a\" + 0.002*\"it\" + 0.002*\"said\"\n"
     ]
    }
   ],
   "source": [
    "train_and_print_topics(comment_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T22:49:17.360322Z",
     "start_time": "2020-12-08T22:46:02.022815Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reading the dataset file, then Preprocessing the comments column \n",
    "\n",
    "df= pd.read_excel('AllFlatEarthComments.xlsx').dropna()\n",
    "\n",
    "df['comment'] = df['comment'].apply(remove_digits)\n",
    "df['comment'] = df['comment'].apply(remove_special_chars)\n",
    "\n",
    "df['comment'] = df['comment'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "df['comment'] = df['comment'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "df['comment'] = df['comment'].apply(lambda x: ' '.join([word.lower() for word in x.split() ]))\n",
    "\n",
    "comment_clean = df['comment']\n",
    "\n",
    "comment_clean = [string.split() for string in comment_clean]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T22:56:24.766120Z",
     "start_time": "2020-12-08T22:49:26.651712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 : 0.998*\"u\" + 0.041*\"f\" + 0.036*\"e\" + 0.026*\"c\" + 0.014*\"n\" + 0.012*\"b\" + 0.008*\"U\" + 0.004*\"xa\" + 0.003*\"cc\" + 0.002*\"r\" + 0.001*\"ufe\" + 0.001*\"ba\" + 0.001*\"fb\" + 0.001*\"da\" + 0.000*\"take\" + 0.000*\"I\" + 0.000*\"af\" + 0.000*\"Bob\" + 0.000*\"earth\" + 0.000*\"g\" + 0.000*\"This\" + 0.000*\"uff\" + 0.000*\"Copy\" + 0.000*\"Google\" + 0.000*\"D\" + 0.000*\"flat\" + 0.000*\"Wiggle\" + 0.000*\"Youtube\" + 0.000*\"de\" + 0.000*\"thing\" + 0.000*\"many\" + 0.000*\"nI\" + 0.000*\"youtube\" + 0.000*\"df\" + 0.000*\"comment\" + 0.000*\"dd\" + 0.000*\"paste\" + 0.000*\"agree\" + 0.000*\"problems\" + 0.000*\"bob\" + 0.000*\"sections\" + 0.000*\"fs\" + 0.000*\"ufd\" + 0.000*\"aa\" + 0.000*\"w\" + 0.000*\"youtu\" + 0.000*\"nhttps\" + 0.000*\"Earth\" + 0.000*\"A\" + 0.000*\"ac\"\n",
      "Topic 2 : 0.744*\"f\" + 0.662*\"U\" + 0.080*\"e\" + -0.040*\"u\" + 0.016*\"c\" + 0.010*\"b\" + 0.008*\"ufe\" + 0.007*\"fb\" + 0.004*\"fc\" + 0.002*\"n\" + 0.001*\"I\" + 0.001*\"earth\" + 0.001*\"flat\" + 0.000*\"The\" + 0.000*\"ec\" + 0.000*\"Earth\" + -0.000*\"Wiggle\" + 0.000*\"ff\" + 0.000*\"fa\" + 0.000*\"ea\" + 0.000*\"people\" + 0.000*\"ee\" + 0.000*\"like\" + 0.000*\"would\" + -0.000*\"xa\" + 0.000*\"see\" + 0.000*\"THE\" + 0.000*\"fe\" + 0.000*\"r\" + 0.000*\"bd\" + 0.000*\"fd\" + 0.000*\"round\" + 0.000*\"one\" + 0.000*\"believe\" + 0.000*\"know\" + 0.000*\"If\" + 0.000*\"think\" + 0.000*\"IS\" + 0.000*\"time\" + 0.000*\"space\" + 0.000*\"ed\" + 0.000*\"eb\" + 0.000*\"A\" + 0.000*\"Flat\" + 0.000*\"sun\" + 0.000*\"video\" + 0.000*\"It\" + 0.000*\"world\" + 0.000*\"say\" + 0.000*\"EARTH\"\n",
      "Topic 3 : 0.707*\"OBAMA\" + 0.707*\"THANKS\" + 0.000*\"xa\" + 0.000*\"THAN\" + 0.000*\"n\" + 0.000*\"Wiggle\" + -0.000*\"u\" + 0.000*\"earth\" + 0.000*\"b\" + 0.000*\"flat\" + 0.000*\"I\" + 0.000*\"The\" + -0.000*\"f\" + 0.000*\"r\" + 0.000*\"xe\" + 0.000*\"Earth\" + -0.000*\"e\" + 0.000*\"der\" + 0.000*\"THE\" + 0.000*\"die\" + 0.000*\"nWiggle\" + 0.000*\"M\" + 0.000*\"see\" + 0.000*\"und\" + 0.000*\"would\" + 0.000*\"wiggle\" + 0.000*\"sun\" + 0.000*\"us\" + 0.000*\"MM\" + 0.000*\"xf\" + 0.000*\"like\" + 0.000*\"Bob\" + 0.000*\"one\" + 0.000*\"nhttps\" + 0.000*\"TO\" + 0.000*\"ist\" + 0.000*\"potato\" + 0.000*\"This\" + 0.000*\"thrown\" + 0.000*\"moon\" + 0.000*\"much\" + 0.000*\"round\" + 0.000*\"Erde\" + 0.000*\"Logan\" + 0.000*\"could\" + 0.000*\"nWhere\" + 0.000*\"song\" + 0.000*\"space\" + 0.000*\"people\" + 0.000*\"A\"\n"
     ]
    }
   ],
   "source": [
    "train_and_print_topics(comment_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T22:57:10.504691Z",
     "start_time": "2020-12-08T22:56:35.781669Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reading the dataset file, then Preprocessing the comments column \n",
    "\n",
    "df= pd.read_excel('AllMoonLandingComments.xlsx').dropna()\n",
    "\n",
    "\n",
    "df['comment'] = df['comment'].apply(remove_digits)\n",
    "df['comment'] = df['comment'].apply(remove_special_chars)\n",
    "\n",
    "\n",
    "df['comment'] = df['comment'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "df['comment'] = df['comment'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "df['comment'] = df['comment'].apply(lambda x: ' '.join([word.lower() for word in x.split() ]))\n",
    "comment_clean = df['comment']\n",
    "\n",
    "comment_clean = [string.split() for string in comment_clean]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T22:58:03.337790Z",
     "start_time": "2020-12-08T22:57:14.074147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 : 0.993*\"u\" + 0.076*\"f\" + 0.066*\"e\" + 0.043*\"c\" + 0.043*\"b\" + 0.029*\"n\" + 0.006*\"r\" + 0.006*\"fb\" + 0.004*\"uff\" + 0.003*\"xb\" + 0.003*\"U\" + 0.002*\"ago\" + 0.002*\"nReply\" + 0.001*\"I\" + 0.001*\"bd\" + 0.001*\"eb\" + 0.001*\"days\" + 0.001*\"fc\" + 0.001*\"cf\" + 0.001*\"moon\" + 0.001*\"ca\" + 0.001*\"eba\" + 0.001*\"af\" + 0.001*\"youtube\" + 0.001*\"dog\" + 0.001*\"fd\" + 0.001*\"views\" + 0.001*\"ed\" + 0.001*\"day\" + 0.001*\"The\" + 0.001*\"db\" + 0.001*\"de\" + 0.001*\"nMinecraft\" + 0.001*\"bf\" + 0.001*\"dd\" + 0.001*\"fa\" + 0.001*\"nSSundee\" + 0.001*\"ae\" + 0.001*\"NASA\" + 0.001*\"every\" + 0.001*\"nhttps\" + 0.001*\"A\" + 0.001*\"ab\" + 0.001*\"www\" + 0.001*\"ea\" + 0.001*\"cd\" + 0.000*\"E\" + 0.000*\"replies\" + 0.000*\"nView\" + 0.000*\"bc\"\n",
      "Topic 2 : -0.748*\"U\" + -0.656*\"f\" + 0.055*\"u\" + -0.034*\"fd\" + -0.029*\"c\" + -0.024*\"fa\" + -0.023*\"moon\" + -0.022*\"e\" + -0.020*\"I\" + -0.018*\"b\" + 0.017*\"n\" + -0.013*\"ufe\" + 0.009*\"youtube\" + -0.008*\"The\" + -0.008*\"would\" + 0.008*\"www\" + -0.008*\"r\" + 0.008*\"com\" + 0.008*\"v\" + 0.007*\"watch\" + -0.007*\"landing\" + 0.006*\"nReply\" + 0.006*\"chT\" + 0.006*\"xqfxVuYhttps\" + 0.006*\"ago\" + -0.006*\"like\" + -0.006*\"NASA\" + -0.005*\"fake\" + -0.005*\"people\" + -0.005*\"one\" + -0.005*\"space\" + -0.005*\"fb\" + -0.004*\"ec\" + -0.004*\"believe\" + -0.004*\"go\" + -0.004*\"see\" + -0.004*\"earth\" + -0.004*\"ee\" + -0.004*\"fc\" + -0.004*\"back\" + -0.004*\"THE\" + -0.004*\"video\" + -0.004*\"It\" + -0.003*\"think\" + -0.003*\"know\" + -0.003*\"Moon\" + -0.003*\"get\" + -0.003*\"even\" + -0.003*\"uff\" + -0.003*\"time\"\n",
      "Topic 3 : -0.993*\"e\" + 0.076*\"f\" + 0.059*\"u\" + 0.042*\"n\" + -0.033*\"U\" + -0.031*\"b\" + 0.028*\"c\" + 0.012*\"fa\" + 0.008*\"r\" + 0.008*\"fb\" + 0.005*\"uff\" + 0.005*\"EARTH\" + 0.005*\"FLAT\" + 0.003*\"nReply\" + 0.003*\"ago\" + 0.003*\"xb\" + -0.003*\"fd\" + 0.002*\"youtube\" + 0.002*\"days\" + 0.002*\"www\" + -0.002*\"de\" + 0.001*\"bd\" + 0.001*\"com\" + 0.001*\"v\" + 0.001*\"watch\" + -0.001*\"dc\" + 0.001*\"eb\" + 0.001*\"cf\" + 0.001*\"dog\" + 0.001*\"views\" + 0.001*\"ed\" + 0.001*\"nhttps\" + 0.001*\"day\" + 0.001*\"eba\" + 0.001*\"fc\" + 0.001*\"ca\" + 0.001*\"nMinecraft\" + -0.001*\"ea\" + 0.001*\"nSSundee\" + 0.001*\"af\" + -0.001*\"ufe\" + -0.001*\"N\" + 0.001*\"replies\" + 0.001*\"nView\" + 0.001*\"bf\" + 0.001*\"E\" + 0.001*\"hours\" + 0.001*\"ae\" + 0.001*\"xf\" + 0.001*\"nRead\"\n"
     ]
    }
   ],
   "source": [
    "train_and_print_topics(comment_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T22:59:56.771460Z",
     "start_time": "2020-12-08T22:58:07.271241Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reading the dataset file, then Preprocessing the comments column \n",
    "\n",
    "df= pd.read_excel('AllVaccinesComments.xlsx').dropna()\n",
    "\n",
    "df['comment'] = df['comment'].apply(remove_digits)\n",
    "df['comment'] = df['comment'].apply(remove_special_chars)\n",
    "\n",
    "\n",
    "df['comment'] = df['comment'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "df['comment'] = df['comment'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "df['comment'] = df['comment'].apply(lambda x: ' '.join([word.lower() for word in x.split() ]))\n",
    "comment_clean = df['comment']\n",
    "\n",
    "comment_clean = [string.split() for string in comment_clean]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T23:02:39.397402Z",
     "start_time": "2020-12-08T23:00:05.250049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 : 0.704*\"f\" + 0.695*\"U\" + 0.138*\"u\" + 0.038*\"n\" + 0.024*\"c\" + 0.015*\"b\" + 0.011*\"e\" + 0.005*\"I\" + 0.002*\"ufe\" + 0.002*\"vaccines\" + 0.002*\"r\" + 0.002*\"bf\" + 0.002*\"vaccine\" + 0.002*\"people\" + 0.001*\"The\" + 0.001*\"fb\" + 0.001*\"bd\" + 0.001*\"get\" + 0.001*\"ab\" + 0.001*\"children\" + 0.001*\"one\" + 0.001*\"www\" + 0.001*\"like\" + 0.001*\"vaccinated\" + 0.001*\"autism\" + 0.001*\"bc\" + 0.001*\"know\" + 0.001*\"would\" + 0.001*\"af\" + 0.001*\"nhttps\" + 0.001*\"child\" + 0.001*\"ac\" + 0.001*\"anti\" + 0.001*\"It\" + 0.001*\"com\" + 0.001*\"even\" + 0.001*\"ba\" + 0.001*\"fc\" + 0.001*\"ae\" + 0.001*\"ad\" + 0.001*\"bb\" + 0.001*\"Dr\" + 0.001*\"kids\" + 0.001*\"measles\" + 0.000*\"ee\" + 0.000*\"disease\" + 0.000*\"ec\" + 0.000*\"cause\" + 0.000*\"fe\" + 0.000*\"think\"\n",
      "Topic 2 : 0.981*\"u\" + -0.115*\"U\" + -0.087*\"f\" + 0.080*\"c\" + 0.076*\"b\" + 0.054*\"n\" + 0.026*\"e\" + 0.021*\"I\" + 0.014*\"bf\" + 0.011*\"r\" + 0.009*\"vaccines\" + 0.009*\"vaccine\" + 0.008*\"bd\" + 0.006*\"people\" + 0.005*\"bc\" + 0.005*\"The\" + 0.005*\"children\" + 0.005*\"ac\" + 0.005*\"get\" + 0.004*\"af\" + 0.004*\"ba\" + 0.004*\"bb\" + 0.004*\"vaccinated\" + 0.004*\"ad\" + 0.004*\"autism\" + 0.004*\"like\" + 0.004*\"www\" + 0.004*\"one\" + 0.004*\"xa\" + 0.003*\"would\" + 0.003*\"ae\" + 0.003*\"cd\" + 0.003*\"know\" + 0.003*\"drug\" + 0.003*\"cc\" + 0.003*\"child\" + 0.003*\"It\" + 0.003*\"even\" + 0.003*\"Dr\" + 0.002*\"FDA\" + 0.002*\"anti\" + 0.002*\"nhttps\" + 0.002*\"measles\" + 0.002*\"disease\" + 0.002*\"time\" + 0.002*\"kids\" + 0.002*\"com\" + 0.002*\"gov\" + 0.002*\"cause\" + 0.002*\"label\"\n",
      "Topic 3 : 0.707*\"TENGO\" + 0.707*\"YO\" + 0.001*\"THE\" + 0.001*\"Y\" + 0.001*\"IMPOSSIBLE\" + 0.001*\"DIFFERENCE\" + 0.001*\"SPOT\" + 0.001*\"YQ\" + 0.000*\"n\" + 0.000*\"r\" + 0.000*\"I\" + 0.000*\"www\" + 0.000*\"nhttps\" + 0.000*\"com\" + -0.000*\"u\" + 0.000*\"vaccines\" + 0.000*\"watch\" + 0.000*\"v\" + 0.000*\"youtube\" + 0.000*\"vaccine\" + 0.000*\"xa\" + 0.000*\"The\" + 0.000*\"people\" + 0.000*\"get\" + 0.000*\"Dr\" + 0.000*\"children\" + 0.000*\"autism\" + 0.000*\"vaccinated\" + 0.000*\"one\" + -0.000*\"f\" + 0.000*\"gov\" + -0.000*\"c\" + 0.000*\"like\" + 0.000*\"even\" + 0.000*\"know\" + 0.000*\"Vaccines\" + 0.000*\"would\" + 0.000*\"A\" + 0.000*\"nih\" + 0.000*\"nlm\" + 0.000*\"ncbi\" + 0.000*\"nhttp\" + 0.000*\"CDC\" + 0.000*\"Autism\" + 0.000*\"Vaccine\" + -0.000*\"b\" + 0.000*\"It\" + 0.000*\"http\" + 0.000*\"pubmed\" + 0.000*\"immune\"\n"
     ]
    }
   ],
   "source": [
    "train_and_print_topics(comment_clean)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
